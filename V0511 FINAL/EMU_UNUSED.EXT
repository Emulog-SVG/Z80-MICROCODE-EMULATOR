'SEPARATE VERSION DID NOT WIN

ASMGO;

'NOW 11.3K,11K FPS
MOV RDI,PQPX;MOV RSI,PBTB;

MOV RY,48;.WWRY:;

MOV R8 ,[RDI      ];MOV R9 ,[RDI+18*8 ];MOV R10,[RDI+36*8 ];MOV R11,[RDI+54*8 ];
MOV R12,[RDI+72*8 ];MOV R13,[RDI+90*8 ];MOV R14,[RDI+108*8];MOV R15,[RDI+126*8];Add RDI,8;

MOV HOR,32;.WWHOR:;
MOV VLA,8;.WWREP:

MOV EBX,R8D;ROR RBX,20;MOV EAX,R12D;And EAX,&HFFFFF;ROR RAX,40;Or RBX,RAX;Shr R8,4;
MOV ECX,R9D;ROR CX,4;ROR RCX,12;MOV BL,CL;MOV RAX,R11;ROR AX,4;MOV CX,AX;ROR RCX,12;ROR RAX,12;MOV BH,AL;ROR RBX,16;
MOV EAX,R10D;ROR AH,4;ROR AX,4;MOV CX,AX;ROR RCX,12;Shr RCX,28;Shr RAX,12;MOV BL,AL;MOV [RSI],RBX;MOV [RSI+8],RCX;

MOV EBX,R9D;ROR RBX,20;MOV EAX,R13D;And EAX,&HFFFFF;ROR RAX,40;Or RBX,RAX;Shr R9,4;
MOV ECX,R10D;ROR CX,4;ROR RCX,12;MOV BL,CL;MOV RAX,R12;ROR AX,4;MOV CX,AX;ROR RCX,12;ROR RAX,12;MOV BH,AL;ROR RBX,16;
MOV EAX,R11D;ROR AH,4;ROR AX,4;MOV CX,AX;ROR RCX,12;Shr RCX,28;Shr RAX,12;MOV BL,AL;MOV [RSI+4096],RBX;MOV [RSI+4104],RCX;

MOV EBX,R10D;ROR RBX,20;MOV EAX,R14D;And EAX,&HFFFFF;ROR RAX,40;Or RBX,RAX;Shr R10,4;
MOV ECX,R11D;ROR CX,4;ROR RCX,12;MOV BL,CL;MOV RAX,R13;ROR AX,4;MOV CX,AX;ROR RCX,12;ROR RAX,12;MOV BH,AL;ROR RBX,16;
MOV EAX,R12D;ROR AH,4;ROR AX,4;MOV CX,AX;ROR RCX,12;Shr RCX,28;Shr RAX,12;MOV BL,AL;MOV [RSI+8192],RBX;MOV [RSI+8200],RCX;

MOV EBX,R11D;ROR RBX,20;MOV EAX,R15D;And EAX,&HFFFFF;ROR RAX,40;Or RBX,RAX;Shr R11,4;
MOV ECX,R12D;ROR CX,4;ROR RCX,12;MOV BL,CL;MOV RAX,R14;ROR AX,4;MOV CX,AX;ROR RCX,12;ROR RAX,12;MOV BH,AL;ROR RBX,16;
MOV EAX,R13D;ROR AH,4;ROR AX,4;MOV CX,AX;ROR RCX,12;Shr RCX,28;Shr RAX,12;MOV BL,AL;MOV [RSI+12288],RBX;MOV [RSI+12296],RCX;ADD RSI,16;

Shr R12,4;Shr R13,4;Shr R14,4;Shr R15,4;

DEC VLA;JNZ .WWREP;

MOV EAX,[RDI      ];ROR RAX,32;Add R8 ,RAX;
MOV EAX,[RDI+18*8 ];ROR RAX,32;Add R9 ,RAX;
MOV EAX,[RDI+36*8 ];ROR RAX,32;Add R10,RAX;
MOV EAX,[RDI+54*8 ];ROR RAX,32;Add R11,RAX;
MOV EAX,[RDI+72*8 ];ROR RAX,32;Add R12,RAX;
MOV EAX,[RDI+90*8 ];ROR RAX,32;Add R13,RAX;
MOV EAX,[RDI+108*8];ROR RAX,32;Add R14,RAX;
MOV EAX,[RDI+126*8];ROR RAX,32;Add R15,RAX;Add RDI,4;

DEC HOR;JNZ .WWHOR;

Sub RDI,32*4;Sub RDI,8;Add RDI,18*8*4;

Sub RSI,256*16;Add RSI,256*16*4;

DEC RY;JNZ .WWRY;

'MOV RAX,RSI;Sub RAX,PBTB;MOV F1,RAX;
'MOV RAX,RDI;Sub RAX,PQPX;MOV F2,RAX;

'JP .WWEND;

'- - - LUTTER
'NOW 5.6K FPS
'FULL 5.2K FPS
MOV R13,PPTOC
MOV R14,PBLR; 
MOV R15,PBTB;
MOV RSI,PPQM;
MOV RDI,PPQMD;
MOV R12,PSS;'Add R12,8*4;'R12=Q1=PSS+(8+RY*256):

MOV R8,49152;
.QQPIX:;  

MOV RBX,[R15];MOVZX RAX,BX;MOV RDX,[RDI+RAX*8];Shr RBX,16;MOVZX RAX,BX;Add RDX,[RDI+RAX*8];Shr RBX,16;MOVZX RAX,BX;Add RDX,[RDI+RAX*8];Shr RBX,16;Add RDX,[RDI+RBX*8];
MOV RBX,[R15+8];MOVZX RAX,BX;Add RDX,[RSI+RAX*8];Shr RBX,16;MOVZX RAX,BX;Add RDX,[RSI+RAX*8];Shr RBX,16;Add RDX,[R13+RBX*8];
MOV EAX,[R14];MOVZX RBX,DX;Shr RDX,16;MOVZX RCX,Byte Ptr[RAX+RBX*2];ROR RCX,8;
MOV BX,DX;Shr RDX,16;MOV CL,[RAX+RBX*2];ROR RCX,8;MOV CL,[RAX+RDX*2];ROL RCX,16;MOV [R12],ECX;Add R12,4;Add R15,16;Add R14,4;


DEC R8;JNZ .QQPIX;

.WWEND:


ADONE:

' - - - - -

JP .WWEND;

'- - - LUTTER
'NOW FPS
MOV R13,PPTOC
MOV R14,PBLR; 
MOV R15,PBTB;
MOV RSI,PPQM;
MOV RDI,PPQMD;
MOV R12,PSS;Add R12,8*4;'R12=Q1=PSS+(8+RY*256):

MOV R8,49152;.QQPIX:;  

MOV RBX,[R15];
MOVZX RAX,BX;MOV RDX,[RDI+RAX*8];Shr RBX,16;
MOVZX RAX,BX;Add RDX,[RDI+RAX*8];Shr RBX,16;
MOVZX RAX,BX;Add RDX,[RDI+RAX*8];Shr RBX,16;Add RDX,[RDI+RBX*8];'GET ALL RBX PPQMD
MOV RBX,[R15+8];
MOVZX RAX,BX;Add RDX,[RSI+RAX*8];Shr RBX,16;
MOVZX RAX,BX;Add RDX,[RSI+RAX*8];Shr RBX,16;Add RDX,[R13+RBX*8];'GET ALL PPQM AND PPTOC
MOV RAX,[R14];MOV EAX,[RAX];
MOVZX RBX,DX;Shr RDX,16;MOVZX RCX,Byte Ptr[RAX+RBX*2];ROR RCX,8;
MOV BX,DX;Shr RDX,16;MOV CL,[RAX+RBX*2];ROR RCX,8;
MOV CL,[RAX+RDX*2];ROL RCX,16;MOV [R12],ECX;Add R12,4;Add R15,16;Add R14,4;
DEC R8;JNZ .QQPIX;

.WWEND:

' OLD STYLE FEED - - - -

MOV RAX,R8;Shr RAX,40;MOVZX RBX,AL;Shl RBX,4;'GET 5TH AS F00
Shr RAX,8;Add RDX,[RSI+RAX*8];'GET 1-4TH AS PPQM

MOV RAX,R12;Shr RAX,40;MOV BL,AL;Shl RBX,4;'GET 5TH AS FF00
Shr RAX,8;Add RDX,[RSI+RAX*8];'GET 1-4TH AS PPQM

MOV RAX,R9;Shr RAX,40;MOV BL,AL;Shl RBX,4;'GET 5TH AS FFF00, 1-4TH AS XFFFX0
MOV RCX,RAX;Shl RCX,8;'GET 1-4TH AS XFFFX000
Shr RAX,16;'GET 1-4TH AS 0XF
MOV BL,AL;Shr RBX,4;'GET ALL AS XFFFF
MOVZX RBX,BX;Add RDX,[RSI+RBX*8];'GET 5-5-5-1 AS PPQM

MOV RAX,R11;Shr RAX,40;MOVZX RBX,AL;Shl RBX,4;'GET 5TH AS F00, 1-4TH AS XFFFX0
Shr RAX,4;MOV CX,AX;Shl RCX,12;'GET 1-4TH AS XFFF-FFFX000 1-4TH AS XFFFX
Shr RAX,4;'GET 1-4TH AS XFFF
MOV BL,AH;Shl RBX,4;'GET 5TH AS FF00

MOV RAX,R10;Shr RAX,44;'GET 1-5TH AS XFFFX0
MOV BL,AL;Shl RBX,4;'GET 5TH AS FFF00
Shr RAX,4;MOV CX,AX;'GET 1-4TH AS XFCFX
ROR CH,4;ROL CX,4;And CL,15;'GET AS FCFX>CFFX>FF0C 
Shr RAX,4;MOV BL,AL;Shr RBX,4;'GET 5TH AS 0FFFF
Add RDX,[RSI+RBX*8];'GET 5-1-5-1 AS PPQM
PINSRQ XMM0,RDX,0;PSRLW XMM0,1;PEXTRQ RDX,XMM0,0;'REDUCE RDX DIV 2
MOVZX RAX,CL;Add RDX,[RDI+RAX*8];'GET C AS PPTOC
Shr RCX,8;MOVZX RAX,CX;MOVZX RAX,CX;Add RDX,[RSI+RAX*8];'GET 3-1 AS PPQM
Shr RCX,16;MOVZX RAX,CX;Add RDX,[RSI+RAX*8];'GET 1-3 AS PPQM

' - - - - 

#Macro SHINE
'PERFORM 5*5 SHINE ADDITION TO EACH PIXEL WITH LUMA REDUCTIONS
'NOW 6.3K,6.9K,6.8K,6.6K,5.8K,5.7K,5.5K,5K FPS
Do:Dim As UShort Ptr PSEL:Dim As ULongInt UF,V0,V1,V2,V3,V4,V5,V6,V7,V8,V9,TT,E1,E2,E3,E4
Dim As ULongInt Z0,Z1,Z2,Z3,Z4,Z5,Z6,Z7,Z8,Z9
Dim As ULongInt M=&HFFFF,MF=&HF,ME=&HFFFEFFFEFFFE,MF0=&HF0,MF00=&HF00,MF000=&HF000,MFFF0=&HFFF0
Dim As ULongInt Ptr UB1,Q2,Q3:Dim As ULong Ptr Q1:Dim As ULong VLA,VLX,VLY,HOR,SA,SB,SC,SD,RY

Q2=PQPX:UB1=PBLR:
' 5 CACHE LINES OF 16*4=U64 TO U64+64 TO SHIFT THEM
'V7-8-9-0 ARE REDUCED PPQMD LUMA AND EXTRA ADD Z1-3PIX+Z2-1PIX OF3*3SURR FIRST HALF AND SECOND BY OFFX +0+1+2+3
'IDEA THAT FAR PIXELS SHINE LESS TWICE, CLOSE PIXELS SHINE FULL, ALL REDUCED BY PCLIM
'PQM LUTS ARE REDUCED BY DIVA SO ITS PARTIAL SHINE ALREADY
'CENTER PIXEL GET ORIGINAL LUMA NOW FULL 25 PIX SHINE, CENTER 3*3 FULL, SURR 5+4+4+3 HALF
'GET LUMA REDUCTION ARRPTR PCLIM IS UNARY COMPONENT SO SEPARATE APPLY AND MAKE RGBA OF SURR 25PIX LUMA

ASMGO;
PINSRQ XMM0,PPTOC,0
PINSRQ XMM0,PPQMC,1
PINSRQ XMM1,UB1,0
MOV RSI,PPQMD;
MOV RDI,PPQML;

MOV RY,192;.QQRY:;'For RY=0 To 191

MOV RAX,PSS;MOV BL,192;Sub BL,Byte Ptr RY;MOVZX RBX,BL;Shl RBX,10;Add RAX,RBX;Add RAX,8*4;PINSRQ XMM1,RAX,1;' IS XMM1-1 Q1=PSS+(8+RY*256):

MOV VLX,16;

MOV RAX,Q2;
MOV R8, [RAX];
MOV R9, [RAX+18*8];
MOV R10,[RAX+36*8];
MOV R11,[RAX+54*8];
MOV R12,[RAX+72*8];

MOV R13,[RAX+90*8];
MOV R14,[RAX+118*8];
MOV R15,[RAX+136*8];

PINSRQ XMM5,[RAX+8],0
PINSRQ XMM6,[RAX+19*8],0
PINSRQ XMM7,[RAX+37*8],0
PINSRQ XMM8,[RAX+55*8],0
PINSRQ XMM9,[RAX+73*8],0

PINSRQ XMM10,[RAX+91*8],0
PINSRQ XMM11,[RAX+119*8],0
PINSRQ XMM12,[RAX+137*8],0

Add RAX,2*8;MOV Q3,RAX;'Q3=Q2+2:

MOV HOR,32;.QQHOR:;'For HOR=1 To 256 Step 8  

MOV VLA,8;.QQREP:''11K FPS IF ONLY PPTOC PIXEL GO

MOV RAX,R8;Shr RAX,40;MOVZX RDX,AL;Shr RAX,8;MOV RBX,[RSI+RAX*8];MOV RAX,R12;Shr RAX,40;MOV DH,AL;Shr RAX,8;ADD RBX,[RSI+RAX*8];And DX,&HF0F0;
MOV RAX,R9;Shr RAX,44;MOVZX RCX,AL;Shr RAX,4;ADD RBX,[RDI+RAX*8];MOV RAX,R11;Shr RAX,44;MOV CH,AL;Shr RAX,4;ADD RBX,[RDI+RAX*8];And CX,&H0F0F;
Add RCX,RDX;ADD RBX,[RSI+RCX*8];MOV RAX,R10;Shr RAX,44;PUSH RAX;Shr RAX,8;MOV RDX,RAX;And RDX,&H0F;PEXTRQ RCX,XMM0,0;Add RBX,[RCX+RDX*8];
Shl RAX,4;POP RCX;MOV AL,CL;MOVZX RAX,AX;PEXTRQ RDX,XMM0,1;Add RBX,[RDX+RAX*8];
PEXTRQ RDX,XMM1,0;MOV RCX,[RDX];MOVZX RDX,BX;MOV DL,[RCX+RDX*2];
ROR RDX,8;ROR RBX,16;MOVZX RAX,BX;MOV DL,[RCX+RAX*2];
ROR RDX,8;ROR RBX,16;MOVZX RAX,BX;MOV DL,[RCX+RAX*2];ROL RDX,16;PEXTRQ RAX,XMM1,1;MOV [RAX],EDX;

MOV RAX,R9;Shr RAX,40;MOVZX RDX,AL;Shr RAX,8;MOV RBX,[RSI+RAX*8];MOV RAX,R13;Shr RAX,40;MOV DH,AL;Shr RAX,8;ADD RBX,[RSI+RAX*8];And DX,&HF0F0;
MOV RAX,R10;Shr RAX,44;MOVZX RCX,AL;Shr RAX,4;ADD RBX,[RDI+RAX*8];MOV RAX,R12;Shr RAX,44;MOV CH,AL;Shr RAX,4;ADD RBX,[RDI+RAX*8];And CX,&H0F0F;
Add RCX,RDX;ADD RBX,[RSI+RCX*8];MOV RAX,R11;Shr RAX,44;PUSH RAX;Shr RAX,8;MOV RDX,RAX;And RDX,&H0F;PEXTRQ RCX,XMM0,0;Add RBX,[RCX+RDX*8];
Shl RAX,4;POP RCX;MOV AL,CL;MOVZX RAX,AX;PEXTRQ RDX,XMM0,1;Add RBX,[RDX+RAX*8];
PEXTRQ RDX,XMM1,0;Add RDX,1024;MOV RCX,[RDX];MOVZX RDX,BX;MOV DL,[RCX+RDX*2];
ROR RDX,8;ROR RBX,16;MOVZX RAX,BX;MOV DL,[RCX+RAX*2];
ROR RDX,8;ROR RBX,16;MOVZX RAX,BX;MOV DL,[RCX+RAX*2];ROL RDX,16;PEXTRQ RAX,XMM1,1;MOV [RAX+1024],EDX;

MOV RAX,R10;Shr RAX,40;MOVZX RDX,AL;Shr RAX,8;MOV RBX,[RSI+RAX*8];MOV RAX,R14;Shr RAX,40;MOV DH,AL;Shr RAX,8;ADD RBX,[RSI+RAX*8];And DX,&HF0F0;
MOV RAX,R11;Shr RAX,44;MOVZX RCX,AL;Shr RAX,4;ADD RBX,[RDI+RAX*8];MOV RAX,R13;Shr RAX,44;MOV CH,AL;Shr RAX,4;ADD RBX,[RDI+RAX*8];And CX,&H0F0F;
Add RCX,RDX;ADD RBX,[RSI+RCX*8];MOV RAX,R12;Shr RAX,44;PUSH RAX;Shr RAX,8;MOV RDX,RAX;And RDX,&H0F;PEXTRQ RCX,XMM0,0;Add RBX,[RCX+RDX*8];
Shl RAX,4;POP RCX;MOV AL,CL;MOVZX RAX,AX;PEXTRQ RDX,XMM0,1;Add RBX,[RDX+RAX*8];
PEXTRQ RDX,XMM1,0;Add RDX,2048;MOV RCX,[RDX];MOVZX RDX,BX;MOV DL,[RCX+RDX*2];
ROR RDX,8;ROR RBX,16;MOVZX RAX,BX;MOV DL,[RCX+RAX*2];
ROR RDX,8;ROR RBX,16;MOVZX RAX,BX;MOV DL,[RCX+RAX*2];ROL RDX,16;PEXTRQ RAX,XMM1,1;MOV [RAX+2048],EDX;

MOV RAX,R11;Shr RAX,40;MOVZX RDX,AL;Shr RAX,8;MOV RBX,[RSI+RAX*8];MOV RAX,R15;Shr RAX,40;MOV DH,AL;Shr RAX,8;ADD RBX,[RSI+RAX*8];And DX,&HF0F0;
MOV RAX,R12;Shr RAX,44;MOVZX RCX,AL;Shr RAX,4;ADD RBX,[RDI+RAX*8];MOV RAX,R14;Shr RAX,44;MOV CH,AL;Shr RAX,4;ADD RBX,[RDI+RAX*8];And CX,&H0F0F;
Add RCX,RDX;ADD RBX,[RSI+RCX*8];MOV RAX,R13;Shr RAX,44;PUSH RAX;Shr RAX,8;MOV RDX,RAX;And RDX,&H0F;PEXTRQ RCX,XMM0,0;Add RBX,[RCX+RDX*8];
Shl RAX,4;POP RCX;MOV AL,CL;MOVZX RAX,AX;PEXTRQ RDX,XMM0,1;Add RBX,[RDX+RAX*8];
PEXTRQ RDX,XMM1,0;Add RDX,3072;MOV RCX,[RDX];MOVZX RDX,BX;MOV DL,[RCX+RDX*2];
ROR RDX,8;ROR RBX,16;MOVZX RAX,BX;MOV DL,[RCX+RAX*2];
ROR RDX,8;ROR RBX,16;MOVZX RAX,BX;MOV DL,[RCX+RAX*2];ROL RDX,16;PEXTRQ RAX,XMM1,1;MOV [RAX+3072],EDX;Add RAX,4;PINSRQ XMM1,RAX,1;
PEXTRQ RDX,XMM1,0;Add RDX,8;PINSRQ XMM1,RDX,0;

Shl R8,4;Shl R9,4;Shl R10,4;Shl R11,4;Shl R12,4;
Shl R13,4;Shl R14,4;Shl R15,4;

DEC VLA;JNZ .QQREP;

Xor RAX,RAX;
PEXTRD EAX,XMM5,1;Add R8 ,RAX;PSLLQ XMM5,32;
PEXTRD EAX,XMM6,1;Add R9 ,RAX;PSLLQ XMM6,32;
PEXTRD EAX,XMM7,1;Add R10,RAX;PSLLQ XMM7,32;
PEXTRD EAX,XMM8,1;Add R11,RAX;PSLLQ XMM8,32;
PEXTRD EAX,XMM9,1;Add R12,RAX;PSLLQ XMM9,32;

PEXTRD EAX,XMM10,1;Add R13,RAX;PSLLQ XMM10,32;
PEXTRD EAX,XMM11,1;Add R14,RAX;PSLLQ XMM11,32;
PEXTRD EAX,XMM12,1;Add R15,RAX;PSLLQ XMM12,32;


Sub VLX,8;JNZ .QQOV
MOV VLX,16;
MOV RAX,Q3;
PINSRQ XMM5,[RAX],0
PINSRQ XMM6,[RAX+18*8],0
PINSRQ XMM7,[RAX+36*8],0
PINSRQ XMM8,[RAX+54*8],0
PINSRQ XMM9,[RAX+72*8],0

PINSRQ XMM10,[RAX+90*8],0
PINSRQ XMM11,[RAX+118*8],0
PINSRQ XMM12,[RAX+136*8],0

Add RAX,8;MOV Q3,RAX;

.QQOV:;

DEC HOR;JNZ .QQHOR;'Next:

MOV RAX,Q2;Add RAX,18*8*4;MOV Q2,RAX;'Q2+=18:

SUB RY,4;JNZ .QQRY;'DUM:Next:

ADONE:



PPTOC=GETPTR(16*8,         32) ' LUT RGB48 OF ZX COLOURS,INKS
PPQM =GETPTR(16*16*16*16*8,32) ' LUT QUAD 4BPP RGB48 SUMM,DIVIDED BY DIVA
PPQMD=GETPTR(16*16*16*16*8,32) ' SHR 1 LLLL VERSION LUT QUAD 4BPP RGB48 SUMM,DIVIDED BY DIVA
PPQML=GETPTR(16*16*16*16*8,32) ' NLLL VERSION LUT QUAD 4BPP RGB48 SUMM,DIVIDED BY DIVA
PPQMC=GETPTR(16*16*16*16*8,32) ' NLLN VERSION LUT QUAD 4BPP RGB48 SUMM,DIVIDED BY DIVA



'NO SPEED CRITICAL
'PLUS SHR 1 VERSION PPQMD
'ON-CRT PIXELS DO SHINE OVER OF OTHER 8 AROUND, TO ACCELERATE USE PRESUMMED 4X FOR EACH 3x3
'MAKE PQM LUT 4BPP*4,GET 4*RGB48 SUMM REDUCED BY DIVA(CUT LUMA SURR 3*3),TO PQM(16*16*16*16)=65536*RGB48
Do:Dim As ULong TI,ER,EG,EB,C1,C2,C3,C4:Dim As ULongInt PMX,QMX,MSK=&H00001FFF1FFF1FFF:TI=0:For C1=0 To 15:For C2=0 To 15:For C3=0 To 15:For C4=0 To 15:

'USUAL NNNN LUMAS PPQM
QMX=*(PPTOC+C1)+*(PPTOC+C2)+*(PPTOC+C3)+*(PPTOC+C4):
ER=((QMX SHR 32) And 65535)/DIVA:EG=((QMX SHR 16) And 65535)/DIVA:EB=(QMX And 65535)/DIVA:
QMX=(ER SHL 32) Or (EG Shl 16) Or EB:'*(PPQM+TI)=QMX:

'ALL LLLL LUMAS PPQMD
QMX=*(PPTOC+C1)+*(PPTOC+C2)+*(PPTOC+C3)+*(PPTOC+C4):
ER=((QMX SHR 32) And 65535)/DIVA:EG=((QMX SHR 16) And 65535)/DIVA:EB=(QMX And 65535)/DIVA:
QMX=((((ER SHL 32) Or (EG Shl 16) Or EB)SHR 1)AND MSK):*(PPQMD+TI)=QMX:

'NEW LNNN LUMAS PPQML
QMX=*(PPTOC+C2)+*(PPTOC+C3)+*(PPTOC+C4):
ER=((QMX SHR 32) And 65535)/DIVA:EG=((QMX SHR 16) And 65535)/DIVA:EB=(QMX And 65535)/DIVA:
PMX=((((ER SHL 32) Or (EG Shl 16) Or EB)Shr 1)And MSK):
QMX=*(PPTOC+C1)
ER=((QMX SHR 32) And 65535)/DIVA:EG=((QMX SHR 16) And 65535)/DIVA:EB=(QMX And 65535)/DIVA:
QMX=PMX+((((ER SHL 32) Or (EG Shl 16) Or EB)Shr 1)And MSK):*(PPQML+TI)=QMX:

'NEW LNNL LUMAS PPQMC
QMX=*(PPTOC+C2)+*(PPTOC+C3)
ER=((QMX SHR 32) And 65535)/DIVA:EG=((QMX SHR 16) And 65535)/DIVA:EB=(QMX And 65535)/DIVA:
PMX=((((ER SHL 32) Or (EG Shl 16) Or EB)Shr 1)And MSK):
QMX=*(PPTOC+C1)+*(PPTOC+C4):
ER=((QMX SHR 32) And 65535)/DIVA:EG=((QMX SHR 16) And 65535)/DIVA:EB=(QMX And 65535)/DIVA:
QMX=PMX+((((ER SHL 32) Or (EG Shl 16) Or EB)Shr 1)And MSK):*(PPQMC+TI)=QMX:

TI=TI+1:Next:Next:Next:Next:Loop While 0


Q2=PQPX:UB1=PBLR:
ASMGO;

MOV RDI,PPTOC;
MOV RSI,PPQMD;
MOV R14,PPQML;
MOV R15,PPQMC;

MOV RY,192;.QQRY:;'For RY=0 To 191

'Z0=*(Q2   ):Z1=*(Q2+18):Z2=*(Q2+36):Z3=*(Q2+54):Z4=*(Q2+72):
MOV RAX,Q2;
MOV R8, [RAX];
MOV R9, [RAX+18*8];
MOV R10,[RAX+36*8];
MOV R11,[RAX+54*8];
MOV R12,[RAX+72*8];

'Z5=*(Q2+1 ):Z6=*(Q2+19):Z7=*(Q2+37):Z8=*(Q2+55):Z9=*(Q2+73):
MOV RBX,[RAX+8   ];MOV Z5,RAX;
MOV RBX,[RAX+19*8];MOV Z6,RAX;
MOV RBX,[RAX+37*8];MOV Z7,RAX;
MOV RBX,[RAX+55*8];MOV Z8,RAX;
MOV RBX,[RAX+73*8];MOV Z9,RAX;

Add RAX,2*8;MOV Q3,RAX;'Q3=Q2+2:

MOV VLX,16;'VLX=16:

MOV RAX,PSS;MOVZX RBX,WORD Ptr RY;Shl RBX,10;Add RAX,RBX;Add RAX,8;MOV Q1,RAX;'Q1=PSS+(8+RY*256):

MOV HOR,32;.QQHOR:;'For HOR=1 To 256 Step 8

MOV VLA,8;.QQREP:'For VLA=0 To 7

MOV RAX,R8;Shr RAX,44;MOVZX RDX,AL;Shr RAX,4;MOV RBX,[RSI+RAX*8];'V7+=*(PPQMD+((Z0 Shr 48)))
MOV RAX,R12;Shr RAX,44;MOV DH,AL;Shr RAX,4;ADD RBX,[RSI+RAX*8];And DX,&H0F0F;'V7+=*(PPQMD+((Z4 Shr 48)))
MOV RAX,R9;Shr RAX,44;MOVZX RCX,AL;Shr RAX,4;ADD RBX,[RSI+RAX*8];'V7+=*(PPQMD+((Z1 Shr 48)And MF000)+((Z1 Shr 36)And MF00)+((Z2 Shr 56)And MF0)+((Z2 Shr 44)And MF))
MOV RAX,R11;Shr RAX,44;MOV CH,AL;Shr RAX,4;ADD RBX,[RSI+RAX*8];Shl CX,4;And CX,&HF0F0;'V7+=*(PPQMD+((Z0 Shr 32)And MF000)+((Z3 Shr 52)And MF00)+((Z3 Shr 40)And MF0)+((Z4 Shr 44)And MF))
Add RCX,RDX;ADD RBX,[RSI+RCX*8];'V7+=*(PPQM+(((Z1 Shr 44) And MFFF0)+((Z2 Shr 56)And MF)))
MOV RAX,R10;Shr RAX,44;MOVZX RCX,AL;Shr RAX,8;MOVZX RDX,AL;And DL,&H0F;Add RBX,[RDI+RDX*8];'V7=*(PPTOC+((Z2 Shr 52) And MF))
Shl RAX,4;MOV AL,CL;MOVZX RAX,AX;Add RBX,[R15+RAX*8];'V7+=*(PPQM+(((Z3 Shr 44) And MFFF0)+((Z2 Shr 48) And MF)))

ShL R8,4;ShL R9,4;ShL R10,4;ShL R11,4;'Z0=(Z0 Shl 4):Z1=(Z1 Shl 4):Z2=(Z2 Shl 4):Z3=(Z3 Shl 4):Z4=(Z4 Shl 4):

MOV RDX,UB1;MOV RCX,[RDX];Add RDX,8;MOV UB1,RDX;'PSEL=*UB1:UB1+=1:
MOVZX RDX,BX;MOV DX,[RCX+RDX*2];'*Q1=(*(PSEL+((V7 Shr 32) And M))Shl 16)
ROR RDX,16;ROR RBX,16;MOVZX RAX,BX;MOV DX,[RCX+RAX*2];
ROR RDX,16;ROR RBX,16;MOVZX RAX,BX;MOV DX,[RCX+RAX*2];ROR RDX,16;'+(*(PSEL+((V7 Shr 16) And M))Shl 8)+*(PSEL+(V7 And M)):Q1+=1:
MOV RAX,Q1;MOV [RAX],RDX;Add RAX,8;MOV Q1,RAX;

DEC VLA;JNZ .QQREP;'Next

'Z0+=(Z5 Shr 32):Z5=(Z5 Shl 32):Z1+=(Z6 Shr 32):Z6=(Z6 Shl 32):Z2+=(Z7 Shr 32):Z7=(Z7 Shl 32):Z3+=(Z8 Shr 32):Z8=(Z8 Shl 32):Z4+=(Z9 Shr 32):Z9=(Z9 Shl 32):
MOV RAX,Z5;MOV RBX,RAX;SHR RAX,32;ADD R8 ,RAX;MOV Z0,R8 ;Shl RBX,32;MOV Z5,RBX;
MOV RAX,Z6;MOV RBX,RAX;Shr RAX,32;Add R9 ,RAX;MOV Z1,R9 ;Shl RBX,32;MOV Z6,RBX;
MOV RAX,Z7;MOV RBX,RAX;Shr RAX,32;Add R10,RAX;MOV Z2,R10;Shl RBX,32;MOV Z7,RBX;
MOV RAX,Z8;MOV RBX,RAX;Shr RAX,32;Add R11,RAX;MOV Z3,R11;Shl RBX,32;MOV Z8,RBX;
MOV RAX,Z9;MOV RBX,RAX;Shr RAX,32;Add R12,RAX;MOV Z4,R12;Shl RBX,32;MOV Z9,RBX;

Sub VLX,8;'VLX=VLX-8:

' SHIFT UPDATE COUNTER THEN UPLOAD RIGHT PARTS
Xor RAX,RAX;Add EAX,VLX;JNZ .QQOV

'If VLX=0 Then VLX=16:Z5=*(Q3):Z6=*(Q3+18):Z7=*(Q3+36):Z8=*(Q3+54):Z9=*(Q3+72):Q3+=1
MOV VLX,16;
MOV RAX,Q1;
MOV RBX,[RAX];MOV Z5,RBX;
MOV RBX,[RAX+18*8];MOV Z6,RBX;
MOV RBX,[RAX+36*8];MOV Z7,RBX;
MOV RBX,[RAX+54*8];MOV Z8,RBX;
MOV RBX,[RAX+72*8];MOV Z9,RBX;

.QQOV:;

DEC HOR;JNZ .QQHOR;'Next:

MOV RAX,Q2;Add RAX,18*8;MOV Q2,RAX;'Q2+=18:

DEC RY;JNZ .QQRY;'DUM:Next:


#Macro SHINE
'PERFORM 5*5 SHINE ADDITION TO EACH PIXEL WITH LUMA REDUCTIONS
' NOW 5K FPS
Do:Dim As UShort Ptr PSEL
Dim As UBYTE Ptr SB1,SB5
Dim As ULongInt UF,V0,V1,V2,V3,V4,V5,V6,V7,V8,V9,TT,E1,E2,E3,E4
Dim As ULongInt Z0,Z1,Z2,Z3,Z4,Z5,Z6,Z7,Z8,Z9
Dim As ULongInt M=&HFFFF,MF=&HF,ME=&HFFFEFFFEFFFE,MF0=&HF0,MF00=&HF00,MF000=&HF000,MFFF0=&HFFF0
Dim As ULongInt Ptr UB1,Q2,Q3
Dim As ULong Ptr Q1
Dim As ULong VLA,VLX,VLY,HOR,SA,SB,SC,SD,RY

Q2=PQPX:' PTR TO PACK 6912 AS 4BPP 258*194, STROKE IS 18*U64
UB1=PBLR:' U64 PTRS TO SELECT BASE ARRAY OF LUMA REDUCTIONS PCLIM OR PPLIM OR PMLIM

'SB5=PBTB+256+1 ' TEST PBTB FILLER

DUM:For RY=0 To 191
Z0=*(Q2):Z5=*(Q2+1):' 5 CACHE LINES OF 16*4=U64 TO U64+64 TO SHIFT THEM 
Z1=*(Q2+18):Z6=*(Q2+19):'1ST AND 5TH LINES IN E, 2,3,4TH LINES IN V, LIKE PREV 3*3 NAMING
Z2=*(Q2+36):Z7=*(Q2+37):
Z3=*(Q2+54):Z8=*(Q2+55):
Z4=*(Q2+72):Z9=*(Q2+73):

VLX=16:
Q1=PSS+(7+RY*256): 
Q3=Q2+2:

DUM:For HOR=1 To 256 Step 4  
'PPQM LUT QUAD 4BPP RGB48 SUMM,DIVIDED BY DIVA

V7=*(PPQM+((Z0 Shr 48)))' GET 4*4BPP FROM Z0 THEN LUT RGB48
V7+=*(PPQM+((Z4 Shr 48)))' SAME FROM Z4
V7+=*(PPQM+((Z1 Shr 48)And MF000)+((Z1 Shr 36)And MF00)+((Z2 Shr 56)And MF0)+((Z2 Shr 44)And MF))
' TOTAL SUMM 12 PIXELS, GET FROM Z1 AND Z2 BY 2 PIXELS TOPLEFT AND TOPRIGHT 

V8=*(PPQM+((Z0 Shr 44) And M))' SUMM 12PIX OF Z0-Z4-Z1-Z2 OFFX X+1 
V8+=*(PPQM+((Z4 Shr 44) And M))
V8+=*(PPQM+((Z1 Shr 44)And MF000)+((Z1 Shr 32)And MF00)+((Z2 Shr 52)And MF0)+((Z2 Shr 40)And MF))

V9=*(PPQM+((Z0 Shr 40) And M))' SUMM 12PIX OFFX+2
V9+=*(PPQM+((Z4 Shr 40) And M))
V9+=*(PPQM+((Z1 Shr 40)And MF000)+((Z1 Shr 28)And MF00)+((Z2 Shr 48)And MF0)+((Z2 Shr 36)And MF))

V0=*(PPQM+((Z0 Shr 36) And M))' SUMM 12PIX OFFX+3
V0+=*(PPQM+((Z4 Shr 36) And M))
V0+=*(PPQM+((Z1 Shr 36)And MF000)+((Z1 Shr 24)And MF00)+((Z2 Shr 44)And MF0)+((Z2 Shr 32)And MF))

'SUMM +4PIX SO +16PIX DONE, TOP 5TH LOW 5TH Z3 TOPLEFT AND TOPRIGHT OFFX+0 +1 +2 +3
V7+=*(PPQM+((Z0 Shr 32)And MF000)+((Z3 Shr 52)And MF00)+((Z3 Shr 40)And MF0)+((Z4 Shr 44)And MF))
V8+=*(PPQM+((Z0 Shr 28)And MF000)+((Z3 Shr 48)And MF00)+((Z3 Shr 36)And MF0)+((Z4 Shr 40)And MF))
V9+=*(PPQM+((Z0 Shr 24)And MF000)+((Z3 Shr 44)And MF00)+((Z3 Shr 32)And MF0)+((Z4 Shr 36)And MF))
V0+=*(PPQM+((Z0 Shr 20)And MF000)+((Z3 Shr 40)And MF00)+((Z3 Shr 28)And MF0)+((Z4 Shr 32)And MF))

'V7-8-9-0 THEN REDUCE LUMA BY /2 AND EXTRA ADD Z1-3PIX+Z2-1PIX OF3*3SURR FIRST HALF AND SECOND BY OFFX +0+1+2+3
' IDEA THAT FAR PIXELS SHINE LESS TWICE, CLOSE PIXELS SHINE FULL, ALL REDUCED BY PCLIM
' SO HAVE FULL 24 PIX COLOUR SUMMS  
V7=((V7 And ME) Shr 1)+*(PPQM+(((Z1 Shr 44) And MFFF0)+((Z2 Shr 56)And MF)))+ *(PPQM+(((Z3 Shr 44) And MFFF0)+((Z2 Shr 48) And MF)))
V8=((V8 And ME) Shr 1)+*(PPQM+(((Z1 Shr 40) And MFFF0)+((Z2 Shr 52)And MF)))+ *(PPQM+(((Z3 Shr 40) And MFFF0)+((Z2 Shr 44) And MF)))
V9=((V9 And ME) Shr 1)+*(PPQM+(((Z1 Shr 36) And MFFF0)+((Z2 Shr 48)And MF)))+ *(PPQM+(((Z3 Shr 36) And MFFF0)+((Z2 Shr 40) And MF)))
V0=((V0 And ME) Shr 1)+*(PPQM+(((Z1 Shr 32) And MFFF0)+((Z2 Shr 44)And MF)))+ *(PPQM+(((Z3 Shr 32) And MFFF0)+((Z2 Shr 36) And MF)))

' PQM LUTS ARE REDUCED BY DIVA SO ITS PARTIAL SHINE ALREADY
' UPDATE SUMMS BY CENTER PIXEL FROM RGB48 LUT, NOW FULL 25 PIX SHINE, CENTER 3*3 FULL, SURR 5+4+4+3 HALF
V7+=*(PPTOC+((Z2 Shr 52) And MF)):
V8+=*(PPTOC+((Z2 Shr 48) And MF)):
V9+=*(PPTOC+((Z2 Shr 44) And MF)):
V0+=*(PPTOC+((Z2 Shr 40) And MF))

' GET LUMA REDUCTION ARRPTR
' PCLIM IS UNARY COMPONENT SO SEPARATE APPLY AND MAKE RGBA OF SURR 25PIX LUMA
' REPEAT FOR OTHER +1+2+3 PIXELS
'Q1+=1:PSEL=PCLIM:
Q1+=1:PSEL=*UB1:
UB1+=1:*Q1=(*(PSEL+((V7 Shr 32) And M))Shl 16)+(*(PSEL+((V7 Shr 16) And M))Shl 8)+*(PSEL+(V7 And M)):
'Q1+=1:PSEL=PCLIM:
Q1+=1:PSEL=*UB1:
UB1+=1:*Q1=(*(PSEL+((V8 Shr 32) And M))Shl 16)+(*(PSEL+((V8 Shr 16) And M))Shl 8)+*(PSEL+(V8 And M))
'Q1+=1:PSEL=PCLIM:
Q1+=1:PSEL=*UB1:
UB1+=1:*Q1=(*(PSEL+((V9 Shr 32) And M))Shl 16)+(*(PSEL+((V9 Shr 16) And M))Shl 8)+*(PSEL+(V9 And M))
'Q1+=1:PSEL=PCLIM:
Q1+=1:PSEL=*UB1:
UB1+=1:*Q1=(*(PSEL+((V0 Shr 32) And M))Shl 16)+(*(PSEL+((V0 Shr 16) And M))Shl 8)+*(PSEL+(V0 And M))

' WRITE TO *PSS ALL PIXELS
'Q1+=1:*Q1=SA:Q1+=1:*Q1=SB:Q1+=1:*Q1=SC:Q1+=1:*Q1=SD

' SHIFT ACCUMULATED 4BPP PIXELS FOR NEXT
Z0=(Z0 Shl 16)+(Z5 Shr 48):Z5=(Z5 Shl 16):
Z1=(Z1 Shl 16)+(Z6 Shr 48):Z6=(Z6 Shl 16):
Z2=(Z2 Shl 16)+(Z7 Shr 48):Z7=(Z7 Shl 16):
Z3=(Z3 Shl 16)+(Z8 Shr 48):Z8=(Z8 Shl 16):
Z4=(Z4 Shl 16)+(Z9 Shr 48):Z9=(Z9 Shl 16):

' SHIFT UPDATE COUNTER THEN UPLOAD RIGHT PARTS
VLX=VLX-4:If VLX=0 Then VLX=16:Z5=*(Q3):Z6=*(Q3+18):Z7=*(Q3+36):Z8=*(Q3+54):Z9=*(Q3+72):Q3+=1

DUM:Next:Q2+=18:
SB5+=2 ' TEST PBTB FILLER
DUM:Next:
Loop While 0
#EndMacro

ASMGO;
MOV RSI,PPAT;
MOV RDI,PBTB;
MOV R14,PBLR;
MOV R15W,1536;.LLPIX:

MOV RAX,[RDI];
MOVZX RBX,AX;Shr RAX,16;MOV RCX,[RSI+RBX*8];MOV [R14],RCX;
MOVZX RBX,AX;Shr RAX,16;MOV RCX,[RSI+RBX*8];MOV [R14+8],RCX;
MOVZX RBX,AX;Shr RAX,16;MOV RCX,[RSI+RBX*8];MOV [R14+16],RCX;
                       ;MOV RCX,[RSI+RAX*8];MOV [R14+24],RCX;
Add RDI,8;Add R14,32;

Sub R15W;JNZ .LLPIX;

ADONE
'PBTB 
'6144*16 SHOW
'Cls:For T1=0 To 191:For T2=0 To 255:
'T4=*(PBTBW+((T1*512+T2*2)Shr 1))
'If T4>511 Then T4=RGB(255,255,255):PRINT BIN(T4,16);":"; 
'For T3=31 To 0 Step -1:If T4 And 32768 Then 
'PSet(T3+(T2*3),50+T1*3),T4' Else EndIf:T4 SHL=1:Next:
'Next:Next:Sleep



'LAST WORKING TRY REVERSE BYTE FEED ' - - - - - -
'BITS TO SURRS - SCREEN NOW 16BIT 256/1+192+1 3x3 SEEK
'NOW 546K,349K,352K, REPACK 6144 AS PBTB 256*(1+192+1)*13+1+1+1BIT VIA PUNF 1+4+1  
MOV R14,PLIN;
MOV RSI,PBTB;
MOV RDI,PUNF;
MOV R12,MIRR;

MOV R15W,192;.LLVLY:

Xor RAX,RAX;Xor RBX,RBX;
MOV BL,[R14+1];'MOV BL,[R12+RBX];
MOV CL,[R14];'MOV CL,[R12+RCX];
MOV BH,CL;Shl RBX,13;

Shl R15,8;MOV R15B,32;.LLVLX:

Or RAX,RBX;And RAX,&H3FF0;MOVUPD XMM0,[RDI+RAX];MOVUPD [RSI],XMM0;Add RSI,16;Shr RBX,8;MOV RAX,RBX;

Add R14,1;Xor RBX,RBX;
'MOV BL,[R14+1];MOV CL,[R12+RBX];
MOV CL,[R14+1];
MOV BL,[R14];'MOV BL,[R12+RBX];And CL,1;
MOV BH,CL;
Shl RBX,5;

DEC R15B;JNZ .LLVLX;Shr R15,8;

DEC R15W;JNZ .LLVLY;


JP .LLEND
' - - - - - 
'SURRS TO PACKS WRITE BACK
'NOW 198K
MOV RSI,PBTB;'B=PBTB ' IS 256*194*16BIT
MOV R8,&H01FF01FF01FF01FF;
MOV R15B,48;.NNVLY:;
Shl R15,8;MOV R15B,64;.NNVLX:
MOV RAX,[RSI     ];MOV RBX,[RSI+512 ];
MOV RCX,[RSI+1024];LEA R14,[RBX+RAX*8];LEA R14,[RCX+R14*8];MOV [RSI],R14;
MOV RDX,[RSI+1536];LEA R14,[RDX+R14*8];And R14,R8;MOV [RSI+512],R14;
MOV R12,[RSI+2048];LEA R14,[R12+R14*8];And R14,R8;MOV [RSI+1024],R14;
MOV R13,[RSI+2560];LEA R14,[R13+R14*8];And R14,R8;MOV [RSI+1536],R14;Add RSI,8;
DEC R15B;JNZ .NNVLX;Add RSI,2048-512;Shr R15,8;DEC R15B;JNZ .NNVLY;

.LLEND:
ADONE:#EndMacro

' - - - - - -
'BITS TO SURRS - SCREEN NOW 16BIT 256/1+192+1 3x3 SEEK
'NOW 546K,349K,352K, REPACK 6144 AS PBTB 256*(1+192+1)*13+1+1+1BIT VIA PUNF 1+4+1  
MOV R14,PLIN;
MOV RSI,PBTB;
MOV RDI,PUNF;
MOV R12,MIRR;

MOV R15W,192;.LLVLY:

Xor RAX,RAX;Xor RBX,RBX;MOV BL,[R14];MOV BL,[R12+RBX];Shl RBX,5;

Shl R15,8;MOV R15B,32;.LLVLX:

OR RAX,RBX;And RAX,&H3FF0;MOVUPD XMM0,[RDI+RAX];MOVUPD [RSI],XMM0;Add RSI,16;Shr RBX,8;MOV RAX,RBX;

Add R14,1;MOV BL,[R14];MOV BL,[R12+RBX];Shl RBX,5;

DEC R15B;JNZ .LLVLX;Shr R15,8;

DEC R15W;JNZ .LLVLY;




'PUNF NOW 16BIT NOW 1+8+1, 1+4+1 LUT
'PREPARE 10BIT STROKES AS 8BYTE STROKES OF L/*/R - PUNF(TRI-PIX UNFOLDER) 'NO SPEED CRITICAL
'NOW LUT 1+8+1 TO CONTAINS TRAILING BITS 'UNMIRROR HERE 7TH BIT>0TH BYTE
Do:Dim As ULong C1,BB,B9,B8,B7,B6,B5,B4,B3,B2,B1,B0,R9,R8,R7,R6,R5,R4,R3,R2,R1,R0:Dim As UByte Ptr X1:X1=PUNF:For C1=0 To 1023:BB=C1:
B0=BB And 1:BB Shr=1:B1=BB And 1:BB Shr=1:B2=BB And 1:BB Shr=1:B3=BB And 1:BB Shr=1:B4=BB And 1:BB Shr=1:
B5=BB And 1:BB Shr=1:B6=BB And 1:BB Shr=1:B7=BB And 1:BB Shr=1:B8=BB And 1:BB Shr=1:B9=BB And 1:
*(X1+ 0)=(B1 Shl 1)+(B0 Shl 2)+(B2 Shl 0):*(X1+ 2)=(B2 Shl 1)+(B1 Shl 2)+(B3 Shl 0):*(X1+ 4)=(B3 Shl 1)+(B2 Shl 2)+(B4 Shl 0):
*(X1+ 6)=(B4 Shl 1)+(B3 Shl 2)+(B5 Shl 0):*(X1+ 8)=(B5 Shl 1)+(B4 Shl 2)+(B6 Shl 0):*(X1+10)=(B6 Shl 1)+(B5 Shl 2)+(B7 Shl 0):
*(X1+12)=(B7 Shl 1)+(B6 Shl 2)+(B8 Shl 0):*(X1+14)=(B8 Shl 1)+(B7 Shl 2)+(B9 Shl 0):X1+=16:Next:

X1=MIRR:For C1=0 To 255:BB=C1:B0=(BB Shr 7) And 1:B1=(BB Shr 5) And 2:B2=(BB Shr 3) And 4:B3=(BB Shr 1) And 8:
B4=(BB Shl 1) And 16:B5=(BB Shl 3) And 32:B6=(BB Shl 5) And 64:B7=(BB Shl 7) And 128:*(X1+0)=B0+B1+B2+B3+B4+B5+B6+B7:X1+=1:Next:

Loop While 0:


'MOVZX RAX,Byte Ptr[R14  ];MOVZX RAX,[R12+RAX];Shl RAX,5;
'MOVZX RBX,Byte Ptr[R14+1];MOVZX RBX,[R12+RBX];Shl RBX,13;
'MOVZX RCX,Byte Ptr[R14+2];MOVZX RCX,[R12+RCX];Shl RCX,21;
'MOVZX RDX,Byte Ptr[R14+3];MOVZX RDX,[R12+RDX];Shl RDX,29;
'Add RAX,RBX;Add RCX,RDX;Add R13,RAX;Add R13,RCX;


'PATTERN DETECTOR AND MERGER - SCREEN NOW 16BIT 256/1+192+1 3x3 SEEK
'NOW 166235K 
'SINGLE PACKER 352K, REPACK 6144 AS PBTB 256*(1+192+1)*13+1+1+1BIT VIA PUNF 1+4+1  
'SINGLE MERGER 314K
#Macro REPBTB ASMGO;
' - - - - - - 
Xor R13,R13;MOV R13D,PBASE;Add R13,PMEM;MOV R14,PBTB;Add R14,512;MOV R12,PUNF;MOV R11,&B111111000;
MOV R15B,3;.LLTERT:Shl R15,8;MOV R15B,8;.LLVLY:Shl R15,8;MOV R15B,8;.LLVLA:
Xor R10,R10;MOV R10D,[R13];Shl R10,4;Shl R15,8;MOV R15B,8;.LLVLX:
          MOV RAX,R10;And RAX,R11;MOV RAX,[R12+RAX];MOV [R14+00],RAX;
Shr R10,4;MOV RAX,R10;And RAX,R11;MOV RAX,[R12+RAX];MOV [R14+08],RAX;
Shr R10,4;MOV RAX,R10;And RAX,R11;MOV RAX,[R12+RAX];MOV [R14+16],RAX;
Shr R10,4;MOV RAX,R10;And RAX,R11;MOV RAX,[R12+RAX];MOV [R14+24],RAX;
Shr R10,4;MOV RAX,R10;And RAX,R11;MOV RAX,[R12+RAX];MOV [R14+32],RAX;
Shr R10,4;MOV RAX,R10;And RAX,R11;MOV RAX,[R12+RAX];MOV [R14+40],RAX;
Shr R10,4;MOV RAX,R10;And RAX,R11;MOV RAX,[R12+RAX];MOV [R14+48],RAX;
Shr R10,4;MOV RAX,R10;And RAX,R11;MOV RAX,[R12+RAX];MOV [R14+56],RAX;
Add R14,64;Add R13,4;Xor RBX,RBX;MOV EBX,[R13];Shl RBX,4;Or R10,RBX;
DEC R15B;JNZ .LLVLX;Shr R15,8;Add R13,256-32;Add R14,2;DEC R15B;JNZ .LLVLA;Shr R15,8;
Sub R13,2048-32;DEC R15B;JNZ .LLVLY;Shr R15,8;Add R13,2048-256;DEC R15B;JNZ .LLTERT;
' - - - - - 
MOV RSI,PBTB;'B=PBTB ' IS 256*194*16BIT
MOV R8,&H01FF01FF01FF01FF;
MOV R15B,48;.NNVLY:;
Shl R15,8;MOV R15B,64;.NNVLX:
MOV RAX,[RSI     ];MOV RBX,[RSI+512 ];
MOV RCX,[RSI+1024];LEA R14,[RBX+RAX*8];LEA R14,[RCX+R14*8];MOV [RSI],R14;
MOV RDX,[RSI+1536];LEA R14,[RDX+R14*8];And R14,R8;MOV [RSI+512],R14;
MOV R12,[RSI+2048];LEA R14,[R12+R14*8];And R14,R8;MOV [RSI+1024],R14;
MOV R13,[RSI+2560];LEA R14,[R13+R14*8];And R14,R8;MOV [RSI+1536],R14;Add RSI,8;
DEC R15B;JNZ .NNVLX;Add RSI,2048-512;Shr R15,8;DEC R15B;JNZ .NNVLY;

ADONE:#EndMacro

'NOW 65K,65.1K,61.7K,55K,54.5K,42.4K,40K,33K,39K,44K,41K,45K,50K,27K,28K,19K,17K FPS 
'HAVE PBTB, SEEK PATTERNS, FILL PBLR WITH MATCHED PATTERN U64PTRS BY LUT PPAT
'Do:Dim As ULongInt Ptr X:Dim As ULong P1,P2,P3,P4,VLY,VLX:Dim As UByte Ptr B
#Macro REPSEEK
ASMGO;
MOV RSI,PBTB;'B=PBTB ' IS 256*194*16BIT
MOV R8,PBLR;
MOV RDI,PPAT;'LUT PATTERN TO PTR64 VALUE
'MOV R9,&H1FF;
MOV R15B,48;.NNVLY:;'For VLY=0 To 191 STEP 2

Shl R15,8;MOV R15B,64;.NNVLX:'For VLX=0 To 255 STEP 4
PUSH R15

MOV RAX,[RSI     ];Shl RAX,6;
MOV RBX,[RSI+512 ];Shl RBX,3;
MOV RCX,[RSI+1024];
MOV RDX,[RSI+1536];
MOV R13,[RSI+2048];
MOV R12,[RSI+2560];

LEA R15,[RAX+RBX];Add R15,RCX;And R15,&H1FF;MOV R10B,[RDI+R15];Shl R10,8;Shr RAX,16;Shr RBX,16;Shr RCX,16;
LEA R15,[RDX+R15*8];And R15,&H1FF;MOV R11B,[RDI+R15];Shl R11,8;Shr RDX,16;
LEA R15,[R13+R15*8];And R15,&H1FF;MOV R14B,[RDI+R15];Shl R14,8;Shr R13,16;
LEA R15,[R12+R15*8];And R15,&H1FF;MOV R9B,[RDI+R15];Shl R9,8;Shr R12,16;

LEA R15,[RAX+RBX];Add R15,RCX;And R15,&H1FF;MOV R10B,[RDI+R15];Shl R10,8;Shr RAX,16;Shr RBX,16;Shr RCX,16;
LEA R15,[RDX+R15*8];And R15,&H1FF;MOV R11B,[RDI+R15];Shl R11,8;Shr RDX,16;
LEA R15,[R13+R15*8];And R15,&H1FF;MOV R14B,[RDI+R15];Shl R14,8;Shr R13,16;
LEA R15,[R12+R15*8];And R15,&H1FF;MOV R9B,[RDI+R15];Shl R9,8;Shr R12,16;

LEA R15,[RAX+RBX];Add R15,RCX;And R15,&H1FF;MOV R10B,[RDI+R15];Shl R10,8;Shr RAX,16;Shr RBX,16;Shr RCX,16;
LEA R15,[RDX+R15*8];And R15,&H1FF;MOV R11B,[RDI+R15];Shl R11,8;Shr RDX,16;
LEA R15,[R13+R15*8];And R15,&H1FF;MOV R14B,[RDI+R15];Shl R14,8;Shr R13,16;
LEA R15,[R12+R15*8];And R15,&H1FF;MOV R9B,[RDI+R15];Shl R9,8;Shr R12,16;

LEA R15,[RAX+RBX];Add R15,RCX;And R15,&H1FF;MOV R10B,[RDI+R15];MOV [R8],R10D;
LEA R15,[RDX+R15*8];And R15,&H1FF;MOV R11B,[RDI+R15];MOV [R8+256],R11D;
LEA R15,[R13+R15*8];And R15,&H1FF;MOV R14B,[RDI+R15];MOV [R8+512],R14D;
LEA R15,[R12+R15*8];And R15,&H1FF;MOV R9B,[RDI+R15];MOV [R8+768],R9D;

Add RSI,8;Add R8,4;

POP R15
DEC R15B;JNZ .NNVLX;

'Sub RSI,512;
Add RSI,2048-512;
'Sub R8,256;
Add R8,1024-256;

Shr R15,8;DEC R15B;JNZ .NNVLY;'Next:Loop While 0
ADONE:
#EndMacro


'44K VERTICALLY
ASMGO;
MOV RSI,PBTB;'B=PBTB ' IS 256*194*16BIT
MOV RDI,PBLR;'X=PBLR ' IS 256*192*8BIT
MOV R14,PPAT;'LUT PATTERN TO PTR64 VALUE
MOV R13,&H01FF

MOV R15B,64;.NNVLY:;'For VLX=0 To 255 STEP 4

MOV R8D,[RSI];MOV R9D,[RSI+512];
MOVZX RAX,R8B;Shl RAX,3;Add AL,R9B;Shr R8,8;Shr R9,8;
MOVZX RBX,R8B;Shl RBX,3;Add BL,R9B;Shr R8,8;Shr R9,8;
MOVZX RCX,R8B;Shl RCX,3;Add CL,R9B;Shr R8,8;Shr R9,8;
MOVZX RDX,R8B;Shl RDX,3;Add DL,R9B;Add RSI,512;

Shl R15,8;MOV R15B,192;.NNVLX:'For VLY=0 To 191

Add RSI,512;MOV R9D,[RSI];
Shl RAX,3;Add AL,R9B;And RAX,R13;MOV R8B,[R14+RAX];Shl R8,8;Shr R9,8;
Shl RBX,3;Add BL,R9B;And RBX,R13;MOV R8B,[R14+RBX];Shl R8,8;Shr R9,8;
Shl RCX,3;Add CL,R9B;And RCX,R13;MOV R8B,[R14+RCX];Shl R8,8;Shr R9,8;
Shl RDX,3;Add DL,R9B;And RDX,R13;MOV R8B,[R14+RDX];MOV [RDI],R8D;Add RDI,256;

DEC R15B;JNZ .NNVLX;
Sub RDI,49152+4;'GET VERT BACK AND GO RIGHT ++4 
Sub RSI,98304+4;

Shr R15,8;DEC R15B;JNZ .NNVLY;'Next:Loop While 0
ADONE:

Xor RAX,RAX;MOV AL,[R11    ];Shl RAX,9;
Xor RBX,RBX;MOV BL,[R11+258];Shl RBX,6;
Xor RCX,RCX;MOV CL,[R11+516];Shl RCX,3;'P1=*(B+1)Shl 6:P1+=*(B+259)Shl 3:P1+=*(B+517)
Or RAX,RBX;Or RAX,RCX;And AX,511;
MOV RAX,[R14+RAX];MOV [R12],RAX;'*(X+0)=*(PPAT+P1)
Add R11,1;Add R12,8;'X+=1:B+=1:


'PATTERN DETECTOR SCREEN IS 1+256+1/1+192+1 3x3 SEEK
'19500 FPS NOW
'HAVE PBTB, SEEK PATTERNS, FILL PBLR WITH MATCHED PATTERN U64PTRS BY LUT PPAT
Do:Dim As ULongInt Ptr X
Dim As ULong P1,P2,P3,P4,VLY,VLX
Dim As UByte Ptr B
X=PBLR ' IS 256*192*8 U64
B=PBTB ' IS 258*194 
*X=PCLIM:X+=1
*X=PCLIM:X+=1
DUM:For VLY=0 To 191
DUM:For VLX=0 To 63 ' PERFORM SINGLE LINE
P1=(*(B+0)Shl 8)+(*(B+1)Shl 7)+(*(B+2)Shl 6)' GET ALL CLOSE
P2=(*(B+1)Shl 8)+(*(B+2)Shl 7)+(*(B+3)Shl 6)
P3=(*(B+2)Shl 8)+(*(B+3)Shl 7)+(*(B+4)Shl 6)
P4=(*(B+3)Shl 8)+(*(B+4)Shl 7)+(*(B+5)Shl 6)
P1+=(*(B+258)Shl 5)+(*(B+259)Shl 4)+(*(B+260)Shl 3)' GET ALL MIDS
P2+=(*(B+259)Shl 5)+(*(B+260)Shl 4)+(*(B+261)Shl 3)
P3+=(*(B+260)Shl 5)+(*(B+261)Shl 4)+(*(B+262)Shl 3)
P4+=(*(B+261)Shl 5)+(*(B+262)Shl 4)+(*(B+263)Shl 3)
P1+=(*(B+516)Shl 2)+(*(B+517)Shl 1)+*(B+518)' GET ALL FAR
P2+=(*(B+517)Shl 2)+(*(B+518)Shl 1)+*(B+519)
P3+=(*(B+518)Shl 2)+(*(B+519)Shl 1)+*(B+520)
P4+=(*(B+519)Shl 2)+(*(B+520)Shl 1)+*(B+521)
' WRITE INSTEAD OF PCLIM U64 PTRS
*(X+0)=*(PPAT+P1)
*(X+1)=*(PPAT+P2)
*(X+2)=*(PPAT+P3)
*(X+3)=*(PPAT+P4)
X+=4:B+=4:Next:B+=2:Next
Loop While 0



'REPACK 6144 TO LINEAR FORM TO LINEAR TO PBTB (1+256+1)*(1+192+1)*8BPP AS 0/1 BYTELY FOR 3*3 PATTERN ANALYZER VIA PUNF
Do 
Dim As ULongInt Ptr XQ1,XQ2,XQ3
Dim As ULongInt Q
Dim As ULong TERT,VLY,VLA,VLX,C1,C2
Dim As UByte Ptr BB1,BB2
BB2=PBTB+256+1:
For TERT=0 To 6143 Step 2048:
For VLY=0 To 255 Step 32:
For VLA=0 To 2047 Step 256:
BB1=PMEM+PBASE+TERT+VLY+VLA:
XQ2=BB2:BB2+=258:
For VLX=0 To 31 Step 8
XQ3=BB1:BB1+=8:Q=*XQ3

C1=(Q And &HFF)Shl 3:Q Shr=8:XQ1=PUNF+C1:*XQ2=*XQ1:XQ2+=1:
C1=(Q And &HFF)Shl 3:Q Shr=8:XQ1=PUNF+C1:*XQ2=*XQ1:XQ2+=1:
C1=(Q And &HFF)Shl 3:Q Shr=8:XQ1=PUNF+C1:*XQ2=*XQ1:XQ2+=1:
C1=(Q And &HFF)Shl 3:Q Shr=8:XQ1=PUNF+C1:*XQ2=*XQ1:XQ2+=1:
C1=(Q And &HFF)Shl 3:Q Shr=8:XQ1=PUNF+C1:*XQ2=*XQ1:XQ2+=1:
C1=(Q And &HFF)Shl 3:Q Shr=8:XQ1=PUNF+C1:*XQ2=*XQ1:XQ2+=1:
C1=(Q And &HFF)Shl 3:Q Shr=8:XQ1=PUNF+C1:*XQ2=*XQ1:XQ2+=1:
C1=(Q And &HFF)Shl 3:Q Shr=8:XQ1=PUNF+C1:*XQ2=*XQ1:XQ2+=1:
Next:
Next:
Next:
Next:Loop While 0

'REPACK 6144 TO PBTB (1+256+1)*(1+192+1)*8BPP AS 0/1 BYTELY FOR 3*3 PATTERN ANALYZER VIA PUNF
ASMGO;MOV CL,8;
MOV R8,PBTB;Add R8,257;'R8=BB2 'BB2=PBTB+256+1:
MOV R9D,PBASE;Add r9,PMEM;'R9=BB1

MOV R10,3;'R10=TERT COUNTER 'For TERT=0 To 6143 Step 2048:
.LLTERT:
MOV R11,8'R11=VLY COUNTER 'For VLY=0 To 255 Step 32:
.LLVLY:
MOV R12,8'R12=VLA COUNTER 'For VLA=0 To 2047 Step 256:
.LLVLA:
MOV R13,32;'R13=VLX 'For VLX=0 To 31 
.LLVLX:
MOV BL,[R9];Xor RAX,RAX;
RCR BX;RCL RAX,CL;RCR BX;RCL RAX,CL;RCR BX;RCL RAX,CL;RCR BX;RCL RAX,CL;RCR BX;RCL RAX,CL;RCR BX;RCL RAX,CL;RCR BX;RCL RAX,CL;RCR BX;RCL RAX,CL;Shr RAX,7;
MOV QWORD Ptr[R8],RAX;ADD R8,8;
'REPEAT VLX
INC R9;DEC R13;JNZ .LLVLX;ADD R8,2;
'REPEAT VLA
Sub R9,32;Add R9,256;DEC R12;JNZ .LLVLA;
'REPEAT VLY
Sub R9,2048;Add R9,32;DEC R11;JNZ .LLVLY;
'REPEAT TERT
Sub R9,256;Add R9,2048;DEC R10W;JNZ .LLTERT;
ADONE:


'REPACK 6144 TO PBTB (1+256+1)*(1+192+1)*8BPP AS 0/1 BYTELY FOR 3*3 PATTERN ANALYZER VIA PUNF
ASMGO;
'MOV R14,PUNF;
MOV R13D,PBASE;Add R13,PMEM;'R9=BB1
MOV R14,PBTB;Add R14,257;'R8=BB2 'BB2=PBTB+256+1:
MOV R12,&H0101010101010101

MOV R15B,3;'R10=TERT COUNTER 'For TERT=0 To 6143 Step 2048:
.LLTERT:
Shl R15,8;MOV R15B,8;'R11=VLY COUNTER 'For VLY=0 To 255 Step 32:
.LLVLY:
Shl R15,8;MOV R15B,8;'R12=VLA COUNTER 'For VLA=0 To 2047 Step 256:
.LLVLA:
Shl R15,8;MOV R15B,8;'R13=VLX 'For VLX=0 To 31 
.LLVLX:

MOV R11D,[R13];Add R13,4;'80000
MOV R8B,R11B;Shr R11,8;
MOV R9B,R11B;Shr R11,8;
MOV R10B,R11B;Shr R11,8;

MOV AL,R8B;MOV BL,R9B;MOV CL,R10B;MOV DL,R11B;Shl RAX,8;Shl RBX,8;Shl RCX,8;Shl RDX,8;SHR R8B;SHR R9B;SHR R10B;SHR R11B;
MOV AL,R8B;MOV BL,R9B;MOV CL,R10B;MOV DL,R11B;Shl RAX,8;Shl RBX,8;Shl RCX,8;Shl RDX,8;SHR R8B;SHR R9B;SHR R10B;SHR R11B;
MOV AL,R8B;MOV BL,R9B;MOV CL,R10B;MOV DL,R11B;Shl RAX,8;Shl RBX,8;Shl RCX,8;Shl RDX,8;SHR R8B;SHR R9B;SHR R10B;SHR R11B;
MOV AL,R8B;MOV BL,R9B;MOV CL,R10B;MOV DL,R11B;Shl RAX,8;Shl RBX,8;Shl RCX,8;Shl RDX,8;SHR R8B;SHR R9B;SHR R10B;SHR R11B;
MOV AL,R8B;MOV BL,R9B;MOV CL,R10B;MOV DL,R11B;Shl RAX,8;Shl RBX,8;Shl RCX,8;Shl RDX,8;SHR R8B;SHR R9B;SHR R10B;SHR R11B;
MOV AL,R8B;MOV BL,R9B;MOV CL,R10B;MOV DL,R11B;Shl RAX,8;Shl RBX,8;Shl RCX,8;Shl RDX,8;SHR R8B;SHR R9B;SHR R10B;SHR R11B;
MOV AL,R8B;MOV BL,R9B;MOV CL,R10B;MOV DL,R11B;Shl RAX,8;Shl RBX,8;Shl RCX,8;Shl RDX,8;SHR R8B;SHR R9B;SHR R10B;SHR R11B;
MOV AL,R8B;MOV BL,R9B;MOV CL,R10B;MOV DL,R11B;Shl RAX,8;Shl RBX,8;Shl RCX,8;Shl RDX,8;SHR R8B;SHR R9B;SHR R10B;SHR R11B;

And RAX,R12;And RBX,R12;And RCX,R12;And RDX,R12;
MOV QWORD Ptr[R14],RAX;
MOV QWORD Ptr[R14+4],RBX;
MOV QWORD Ptr[R14+8],RCX;
MOV QWORD Ptr[R14+12],RDX;

Add R14,32;

'REPEAT VLX
DEC R15B;JNZ .LLVLX;Shr R15,8;
'REPEAT VLA
Add R13,256-32;Add R14,2;DEC R15B;JNZ .LLVLA;Shr R15,8;
'REPEAT VLY
Sub R13,2048-32;DEC R15B;JNZ .LLVLY;Shr R15,8;
'REPEAT TERT
Add R13,2048-256;DEC R15B;JNZ .LLTERT;
ADONE: